- name: Create PrometheusRule for Alerts
  kubernetes.core.k8s:
    definition:
      apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      metadata:
        name: "{{ app }}-alerts"
        namespace: "{{ openshift_namespace }}"
      spec:
        groups:
          - name: "{{ app }}-sync-alerts"
            rules:
              - alert: "{{ app[0]|upper }}{{ app[1:] }}SyncJobFailed"
                expr: >
                  kube_job_failed{namespace="{{ openshift_namespace }}", job_name="{{ app }}-sync"} > 0
                for: 5m
                labels:
                  severity: critical
                  sendto: telegram
                  app: "{{ app }}"
                annotations:
                  summary: "{{ app }}-sync CronJob failed"
                  description: >-
                    The {{ app }}-sync CronJob has failed. Check logs for details.

              - alert: "{{ app[0]|upper }}{{ app[1:] }}SyncJobDidNotRun"
                expr: >
                  (hour() > 5 or hour() < 2) and
                  (time() - max(kube_job_status_start_time{namespace="{{ openshift_namespace }}", job_name=~"{{ app }}-sync.*"})) > 24 * 3600
                for: 5m
                labels:
                  severity: warning
                  sendto: telegram
                  app: "{{ app }}"
                annotations:
                  summary: "{{ app }}-sync didn't run in its scheduled window"
                  description: >-
                    The {{ app }}-sync CronJob didn't execute during its scheduled window (2h - 5h UTC).

              - alert: "{{ app[0]|upper }}{{ app[1:] }}SyncJobRunningTooLong"
                expr: >
                  time() - kube_job_status_start_time{namespace="{{ openshift_namespace }}", job_name="{{ app }}-sync"} > 3600
                for: 5m
                labels:
                  severity: critical
                  sendto: telegram
                  app: "{{ app }}"
                annotations:
                  summary: "{{ app }}-sync running too long"
                  description: >-
                    The {{ app }}-sync CronJob has been running for more than 1 hour.

              - alert: "{{ app[0]|upper }}{{ app[1:] }}SyncContainerRestarting"
                expr: >
                  increase(kube_pod_container_status_restarts_total{namespace="{{ openshift_namespace }}", container="{{ app }}-sync"}[15m]) > 1
                for: 5m
                labels:
                  severity: warning
                  sendto: telegram
                  app: "{{ app }}"
                annotations:
                  summary: "{{ app }}-sync container restarts detected"
                  description: >-
                    The container running {{ app }}-sync has restarted more than once in the last 15 minutes.

              - alert: "{{ app[0]|upper }}{{ app[1:] }}SyncJobCompletedButFailed"
                expr: >
                  kube_job_complete{namespace="{{ openshift_namespace }}", job_name="{{ app }}-sync"} == 1
                  and kube_job_failed{namespace="{{ openshift_namespace }}", job_name="{{ app }}-sync"} > 0
                for: 5m
                labels:
                  severity: warning
                  sendto: telegram
                  app: "{{ app }}"
                annotations:
                  summary: "{{ app }}-sync completed but had failures"
                  description: >-
                    The {{ app }}-sync job completed but encountered failures. Check logs for details.

              - alert: "{{ app[0]|upper }}{{ app[1:] }}SyncPodRestartingTooOften"
                expr: >
                  increase(kube_pod_container_status_restarts_total{namespace="{{ openshift_namespace }}", pod=~"{{ app }}-sync-.*"}[30m]) > 3
                for: 5m
                labels:
                  severity: critical
                  sendto: telegram
                  app: "{{ app }}"
                annotations:
                  summary: "{{ app }}-sync pod restarting too often"
                  description: >-
                    The {{ app }}-sync pod has restarted more than 3 times in the last 30 minutes.

          - name: "{{ app }}-pod-alerts"
            rules:
              - alert: "{{ app[0]|upper }}{{ app[1:] }}PodRestartingTooOften"
                expr: >
                  increase(kube_pod_container_status_restarts_total{namespace="{{ openshift_namespace }}", pod=~"{{ app }}-.*", pod!~"{{ app }}-(sync|redis|exporter).*"}[30m]) > 3
                for: 5m
                labels:
                  severity: critical
                  sendto: telegram
                  app: "{{ app }}"
                annotations:
                  summary: "{{ app }} application pod restarting too often"
                  description: >-
                    The {{ app }} application pod has restarted more than 3 times in the last 30 minutes.

              - alert: "{{ app[0]|upper }}{{ app[1:] }}PodMissing"
                expr: >
                  absent(kube_pod_info{namespace="{{ openshift_namespace }}", pod=~"{{ app }}-[^(sync|redis|exporter)].*"}) == 1
                for: 15s
                labels:
                  severity: critical
                  sendto: telegram
                  app: "{{ app }}"
                annotations:
                  summary: "{{ app }} application pod is missing"
                  description: >-
                    No {{ app }} application pod is running for more than 15 seconds.

              - alert: "{{ app[0]|upper }}{{ app[1:] }}PodNotReady"
                expr: >
                  sum by (pod) (kube_pod_status_phase{namespace="{{ openshift_namespace }}", pod=~"{{ app }}-[^(sync|redis|exporter)].*", phase=~"Running|Pending"}) > 0
                  and
                  sum by (pod) (kube_pod_status_ready{namespace="{{ openshift_namespace }}", pod=~"{{ app }}-[^(sync|redis|exporter)].*", condition="true"}) == 0
                for: 45s
                labels:
                  severity: critical
                  sendto: telegram
                  app: "{{ app }}"
                annotations:
                  summary: "{{ app }} application pod is not ready"
                  description: >-
                    The {{ app }} application pod is running but not ready for more than 15 seconds.

              - alert: "{{ app[0]|upper }}{{ app[1:] }}PodCrashLoopBackOff"
                expr: >
                  sum by (pod) (kube_pod_container_status_waiting_reason{namespace="{{ openshift_namespace }}", pod=~"{{ app }}-[^(sync|redis|exporter)].*", reason="CrashLoopBackOff"}) > 0
                for: 15s
                labels:
                  severity: critical
                  sendto: telegram
                  app: "{{ app }}"
                annotations:
                  summary: "{{ app }} pod is in CrashLoopBackOff"
                  description: >-
                    The {{ app }} application pod is in CrashLoopBackOff state for more than 15 seconds.

          - name: "{{ app }}-exporter"
            rules:
              - alert: "{{ app[0]|upper }}{{ app[1:] }}ExporterPodMissing"
                expr: >
                  absent(kube_pod_info{namespace="{{ openshift_namespace }}", pod=~"{{ app }}-exporter-.*"})
                for: 1m
                labels:
                  severity: warning
                  sendto: telegram
                  app: "{{ app }}"
                annotations:
                  summary: "{{ app }} Exporter pod is missing"
                  description: >-
                    No {{ app }} Exporter pod is running for more than 1 minute.

              - alert: "{{ app[0]|upper }}{{ app[1:] }}ExporterPodNotReady"
                expr: >
                  sum by (pod) (kube_pod_status_phase{namespace="{{ openshift_namespace }}", pod=~"{{ app }}-exporter-.*", phase=~"Running|Pending"}) > 0
                  and
                  sum by (pod) (kube_pod_status_ready{namespace="{{ openshift_namespace }}", pod=~"{{ app }}-exporter-.*", condition="true"}) == 0
                for: 1m
                labels:
                  severity: warning
                  sendto: telegram
                  app: "{{ app }}"
                annotations:
                  summary: "{{ app }} Exporter pod is not ready"
                  description: >-
                    The {{ app }} Exporter pod is running but not ready for more than 1 minute.
              - alert: "{{ app[0]|upper }}{{ app[1:] }}ExporterPodCrashLoopBackOff"
                expr: >
                  sum by (pod) (kube_pod_container_status_waiting_reason{namespace="{{ openshift_namespace }}", pod=~"{{ app }}-exporter-.*", reason="CrashLoopBackOff"}) > 0
                for: 2m
                labels:
                  severity: warning
                  sendto: telegram
                  app: "{{ app }}"
                annotations:
                  summary: "{{ app }} Exporter pod is in CrashLoopBackOff"
                  description: >-
                    The {{ app }} Exporter pod is in CrashLoopBackOff state. Check pod logs and events.

          - name: "{{ app }}-redis-alerts"
            rules:
              - alert: "{{ app[0]|upper }}{{ app[1:] }}RedisPodMissing"
                expr: >
                  absent(kube_pod_info{namespace="{{ openshift_namespace }}", pod=~"{{ app }}-redis-.*"})
                for: 15s
                labels:
                  severity: critical
                  sendto: telegram
                  app: "{{ app }}"
                annotations:
                  summary: "{{ app }} Redis pod is missing"
                  description: >-
                    No Redis pod for {{ app }} is running for more than 15 seconds.

              - alert: "{{ app[0]|upper }}{{ app[1:] }}RedisPodNotReady"
                expr: >
                  sum by (pod) (kube_pod_status_phase{namespace="{{ openshift_namespace }}", pod=~"{{ app }}-redis-.*", phase=~"Running|Pending"}) > 0
                  and
                  sum by (pod) (kube_pod_status_ready{namespace="{{ openshift_namespace }}", pod=~"{{ app }}-redis-.*", condition="true"}) == 0
                for: 15s
                labels:
                  severity: critical
                  sendto: telegram
                  app: "{{ app }}"
                annotations:
                  summary: "{{ app }} Redis pod is not ready"
                  description: >-
                    The {{ app }} Redis pod is running but not ready for more than 15 seconds.

              - alert: "{{ app[0]|upper }}{{ app[1:] }}RedisPodCrashLoopBackOff"
                expr: >
                  sum by (pod) (kube_pod_container_status_waiting_reason{namespace="{{ openshift_namespace }}", pod=~"{{ app }}-redis-.*", reason="CrashLoopBackOff"}) > 0
                for: 2m
                labels:
                  severity: critical
                  sendto: telegram
                  app: "{{ app }}"
                annotations:
                  summary: "{{ app }} Redis pod is in CrashLoopBackOff"
                  description: >-
                    The {{ app }} Redis pod is in CrashLoopBackOff state. Check pod logs and events.

- name: Create Outline Exporter Deployment
  kubernetes.core.k8s:
    definition:
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: "{{ app }}-exporter"
        namespace: "{{ openshift_namespace }}"
        labels:
          app: "{{ app }}"
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: "{{ app }}-exporter"
        template:
          metadata:
            labels:
              app: "{{ app }}-exporter"
              monitoring: "true"
            annotations:
              prometheus.io/scrape: "true"
              prometheus.io/port: "9877"
              prometheus.io/path: "/metrics"
          spec:
            containers:
              - name: outline-exporter
                image: "{{ registry }}/{{ organisation }}/{{ app }}-exporter:1.2.0"
                imagePullPolicy: Always
                ports:
                  - containerPort: 9877
                    name: metrics
                env:
                  - name: OUTLINE_API_URL
                    value: "https://{{ subdomain }}{% if env == 'test' %}-test{% endif %}.{{ domain }}"
                  - name: OUTLINE_API_KEY
                    valueFrom:
                      secretKeyRef:
                        name: "{{ app }}-api-key-monitoring"
                        key: API_KEY
                resources:
                  requests:
                    cpu: "100m"
                    memory: "128Mi"
                  limits:
                    cpu: "200m"
                    memory: "256Mi"
                readinessProbe:
                  httpGet:
                    path: /metrics
                    port: metrics
                  initialDelaySeconds: 5
                  periodSeconds: 10
            imagePullSecrets:
              - name: "{{ pull_secret_name }}"

- name: Create Outline Exporter Service
  kubernetes.core.k8s:
    definition:
      apiVersion: v1
      kind: Service
      metadata:
        name: "{{ app }}-exporter"
        namespace: "{{ openshift_namespace }}"
        labels:
          app: "{{ app }}-exporter"
      spec:
        ports:
          - name: metrics
            port: 9877
            targetPort: metrics
        selector:
          app: "{{ app }}-exporter"

- name: Create ServiceMonitor for Outline Exporter
  kubernetes.core.k8s:
    definition:
      apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      metadata:
        name: "{{ app }}-exporter"
        namespace: "{{ openshift_namespace }}"
        labels:
          app: "{{ app }}-exporter"
          release: prometheus-operator
      spec:
        endpoints:
          - port: metrics
            interval: 5m
            path: /metrics
        selector:
          matchLabels:
            app: "{{ app }}-exporter"
        namespaceSelector:
          matchNames:
            - "{{ openshift_namespace }}"

- name: Deploy Probe for HTTP endpoint monitoring
  kubernetes.core.k8s:
    definition:
      apiVersion: monitoring.rhobs/v1
      kind: Probe
      metadata:
        name: "{{ app }}-http-probe"
        namespace: "{{ openshift_namespace }}"
      spec:
        interval: 30s
        scrapeTimeout: 10s
        module: http_2xx
        prober:
          url: prometheus-blackbox-exporter.monitoring.svc.cluster.local:9115
        targets:
          staticConfig:
            static:
              - https://{{ subdomain }}{% if env == 'test' %}-test{% endif %}.{{ domain }}
        metricRelabelings:
          - sourceLabels: [__address__]
            targetLabel: instance
          - sourceLabels: [__address__]
            targetLabel: target
          - targetLabel: job
            replacement: "{{ app }}-probe"

- name: Create PrometheusRule for Probe Alerts
  kubernetes.core.k8s:
    definition:
      apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      metadata:
        name: "{{ app }}-probe-alerts"
        namespace: "{{ openshift_namespace }}"
      spec:
        groups:
          - name: "{{ app }}-endpoint-alerts"
            rules:
              - alert: "{{ app[0]|upper }}{{ app[1:] }}EndpointDown"
                expr: >
                  probe_success{job="{{ app }}-probe"} == 0
                for: 1m
                labels:
                  severity: critical
                  sendto: telegram
                  app: "{{ app }}"
                annotations:
                  summary: "{{ app }} endpoint is down"
                  description: >-
                    The {{ app }} endpoint https://{{ subdomain }}{% if env == 'test' %}-test{% endif %}.{{ domain }} has been unreachable for over 1 minute.

              - alert: "{{ app[0]|upper }}{{ app[1:] }}SlowResponse"
                expr: >
                  probe_duration_seconds{job="{{ app }}-probe"} > 2
                for: 5m
                labels:
                  severity: warning
                  sendto: telegram
                  app: "{{ app }}"
                annotations:
                  summary: "{{ app }} slow response time"
                  description: >-
                    The {{ app }} endpoint is responding slowly (>2s) for over 5 minutes.

              - alert: "{{ app[0]|upper }}{{ app[1:] }}SSLCertExpiringSoon"
                expr: >
                  probe_ssl_earliest_cert_expiry{job="{{ app }}-probe"} - time() < 86400 * 14
                for: 1h
                labels:
                  severity: warning
                  sendto: telegram
                  app: "{{ app }}"
                annotations:
                  summary: "{{ app }} SSL certificate expiring soon"
                  description: >-
                    The SSL certificate for {{ app }} will expire in less than 14 days.

- name: Deploy Blackbox Exporter ConfigMap
  kubernetes.core.k8s:
    definition:
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: "{{ app }}-blackbox-config"
        namespace: "{{ openshift_namespace }}"
      data:
        blackbox.yml: |
          modules:
            http_2xx:
              prober: http
              timeout: 5s
              http:
                valid_status_codes: [200]
                method: GET
                preferred_ip_protocol: "ip4"
                fail_if_ssl: false
                fail_if_not_ssl: true
                tls_config:
                  insecure_skip_verify: false
